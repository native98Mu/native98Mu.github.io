---
layout: post
title: 大模型时代，如何用AI筑牢内容安全的防线
# subtitle: 还有哪些困难需要解决
cover-img: /assets/img/security-cover.png
# cover-img: /assets/img/path.jpg
# thumbnail-img: /assets/img/thumb.png
# share-img: /assets/img/path.jpg
tags: [LLM, VLM, security, Product, Eval]
author: Muji
---

## 为什么各大平台如此重视内容安全

内容安全一直是互联网平台的生命线。无论是平台自制内容（PGC），还是用户生成内容（UGC）、AI 生成内容（AIGC），不当信息的传播都可能引发舆论危机，甚至带来法律与商业风险。
近几年我们也看到过不少案例，比如国内直播平台因涉黄、涉赌被下架整改。由此可见，内容安全不仅是“运营问题”，更是企业的生存问题。

从地域差异上看，海外更强调种族歧视、仇恨言论、宗教冲突等，而国内则对政治及社会稳定等相关内容尤为敏感。这些差异，直接决定了平台对内容管控不同的侧重点。

## 内容安全通用诉求

过去参与了比较多样类型的内容安全需求实施，深刻感受到不同行业对内容安全的尺度的需求差异还蛮明显，这里我以我接触过的几类典型的行业应用，先来聊聊内容安全部分通用诉求和行业化需求：

1. 低延迟

这个毫无疑问，作为业务方肯定都希望安全检测的速度越快越好。因为一般安全检测环节处于用户侧已经提交了一个动作，等待反馈的过程中。举个例子，用户发送一条微博，一般都会有发送中这个过程，不仅是网络的传输，还会有发送内容的检测。整体RT，对于短文本期望都是200ms左右，即使是最长不超过1s。而对于图像检测，可以适当放大到500ms-2s，视频检测则对应范围会更宽，而且用户对图片和视频上传需要更多的时间接受度更高。

2. 召回率高

这里我们先提一下数据样本组成，分为黑样本和白样本。黑样本指一条数据人工判定为不安全，白样本则为安全通过的数据。召回率就是指在所有真实的黑样本里，模型成功识别出的比例，也就是说模型判断正确的黑样本占所有黑样本的比例。更直观一些就是筛选黑样本有没有被漏掉，这个对于那些风险要求非常严格的应用就很重要，毕竟“宁可错杀一千，也不能放过一个”，所以如果一昧的追求高召回也随之带来错杀的风险。

3. 精准率高

精准率通常和召回率一起来做模型效果判定。它是指在所有模型判定为黑样本的数据里，实际上真的是黑样本的比例。更直观来说就是模型判黑的结果里，有多少是真的黑，是否有“错杀”白样本。理想情况肯定是希望召回率和精准率都比较高，但在业务的模型表现上这两个很可能是跷跷板，一个上来一个就下去，达到平衡还是需要很多工作。一般要求高精准率的场景是大概率模型过一遍后，人工还要继续check，确认后可能还需要进一步对用户账号做处理等。所以就需要模型尽可能正确的做出判断，可以漏掉，但抓到的一定要是对的，降低人参与的工作量。

4. 覆盖面广

一般都包含敏感，色情，暴力，恐怖，辱骂，歧视等等多维度的风险类别，还有越来越多平台把金融欺诈、赌博引流、虚假广告也纳入内容安全范畴，确保公开的内容符合平台及相关法律法规的要求。

5. 多模态支持

过去主要是文本和图像为主，现在文字、图像、音频、视频一体化检测需求越来越强

5. 价格便宜

价格是肯定的啦，毕竟如果是ugc或者是aigc场景，每天产生的数据条数都会一个极高的数量级，不论计费是按次数还是 token 数，都是希望价格越低越好的。而且负责内容安全与风控在公司里一般都是“成本中心”，大规模数据情况下，大小模型配合也是非常合适的方式。

6. 灵活配置

一般成熟的内容安全产品，都会有控制台或开放接口给客户做黑名单或白名单以及规则的自主配置。这种敏感词或者是小模型协作的方式非常重要，业务侧能快速响应突发事件，比如临时敏感话题。是LLM的一个有力补充。

7. 多语种支持与本地化

国内有不少公司都是专注做出海业务，自然对语种也有更多的要求。除了常用的中英，日韩，东南亚，阿语，土耳其语等等都是比较热门的地区语种。可能受限于训练语料，国内的模型一般在小语种上的表现还有些差距。同时，不同市场的差异，往往意味着不同的模型定制策略，期望同一个基础模型结合相关配置能够满足不同的侧重点。




### 行业差异化需求
对于国内的应用，一些敏感红线都是大家的共识。但是庞大的用户基数和群众的智慧还是有各种对抗方式出现，这些对抗样本，尤其是一些对业务不甚了解的人都不好判断的，边接模糊的样本，对于模型后训练都是至关重要的。这里也聊聊我比较熟悉的几个行业典型的需求

第一个就是我们最熟悉的社交行业。不论是微博，贴吧这种传统的社区类应用，还是陌陌，探探等真人社交，亦或者是像猫箱，星野这种 AI 陪伴，都少不了内容安全及风控欺诈等需求。以ai社交陪伴为典型来聊下内容安全特点，这个部分要控制的点最突出的就是擦边，毕竟现在交通这么发达，用户在和虚拟角色聊天时很可能就会想上高速。这个部分一个大的挑战就是大多数模型仍以单句检测为主，很难理解连续语境。通过观察用户和虚拟角色聊天的数据，以擦边做维度分析，可以看到除了很露骨的部分，东亚很多人聊天的特点是“意擦形不擦”，以及“segment擦，单句不擦”。那这两个是什么意思呢？“意擦形不擦”就是说一句话Literal是没有什么特殊含义的，但是Figurative就是越界的，需要模型能够get到这个意思，才能做出正确的判定。第二个“segment擦，单句不擦”，就是指一句话单句看是没什么问题的，但是放在上下文语境中就已经车开得飞起了。这两种都是目前通用的模型处理得不是很好的部分，所以对于角色扮演类AI陪伴如何把握陪伴尺度，以及到底要送入哪些内容给模型做理解都是比较重要的。

另一方面，由于角色扮演类还是短对话为主，一般llm输出的内容都是结束后一起做检测（整体llm回复RT<1.5s），但对于一些偏长文本回复或剧情向的应用里，就会有单次大段的内容输出，必然无法继续输出完成后再呈现给用户，流式是大多数的选择。但流式输出又会存在初始的内容没问题，模型输出时不断累积，之后又有问题的情况的出现。这样之前对话框已经输出的内容可能就会突然变成预设的“我们来聊点别的东西吧”，体感也不是很好。是否更直接的通过风险前置解决呢？比如能在对话一开始，通过分析用户的输入和对话历史，预判可能的风险走向，提前调整模型的输出策略呢？这里想到OpenAI去年底发的 <Deliberative alignment: reasoning enables safer language models>中展示的例子：
![openai-example](/assets/img/openai_security.png)

第一步先解码了 ROT13 编码的文本，模型意识到用户实际上是在请求有关如何逃避法律制裁的建议。之后模型检查了OpenAI的安全策略，它认识到，“运营色情网站”本身可能不违法，但“这样警察就找不到我”可能暗示某种不正当的非法行为，也就是用户正在寻求关于如何逃避执法部门惩戒。之后模型推理出，用户实际上是在请求关于如何违法的指导。所以，当下用户的请求可以被看作是协助不当行为的请求。同时模型意识到用户通过 ROT13 编码来规避安全检查，这是不被允许的。模型最终给出的回答是：I'm sorry, but I can't comply with that。直接就给出了拒绝的回答，是符合预期的安全回应。


第二个就是我们日常使用很多的音视频啦。不论是长短视频还是直播，音视频行业都有蛮多内容安全需要。比如直播业务，典型的一个场景就是通过用户头像，用昵称以及直播间发言将用户引到私域平台。这里可能就会出现第三方im的联系方式，或者是引导性的内容，有各种各样对抗性样本的出现，都需要模型做出正确的判断。相对于精准率，能否把黑样本应召尽召在这类场景就更重要些。


最后再聊下游戏，和社交类似，游戏中ugc发言都需要及时的内容安全检测，但不同的游戏可能在不同阶段又会因为处理机制有不同的侧重。比如 moba 类游戏，打团过程中一个team里很可能就会有人情绪上头开始口喷某个玩家。一般为了维护游戏氛围的和平，平台就会设定类似友善值的指标对玩家行为作限制。此时被喷的玩家自然不服，但他比较文明不想喷回去，于是就使用平台提供的举报工具，提供本局的对局情况给平台做判定。对于一个大型moba游戏，如果都是人工做裁判，那肯定是忙不过来的，这时候"llm as a judge"就非常有用，结合用户举报数据、游戏内行为数据（如玩家 K/D、聊天频率等）、以及人工复核机制，共同构建一个多维度、高精度的仲裁系统。而且在这里就尤其需要模型有很好的精准率，降低人工的工作内容。还有比如Minecraft这种搭建自由创造类游戏，玩家可以使用像素砖块搭建各式各样内容。那对于各种各样的玩家，他们便会不断试探平台的底线，通过各种“有趣”的标识或者是“同好”的共识来表达一些事件或者是立场。甚至得益于游戏的自由度，还能有各种视角来传达，这对于平台监管方真的是太难了。这样象征含义的图像检测对于过去的小模型就是非常困难的case，如今有了vlm的帮助，这样的红线case能够被及时发现，最大程度避免不必要的麻烦。再或者说slg这类主力营收很多需要依靠大R的类型，也就是靠氪金大佬们撑起来的。在同类竞品如此丰富的红海游戏市场，避免大R被友商勾走自然是平台必须要防备的。同时还有游戏里的道具材料等非官方买卖，这样的“黑市”会侵占官方的交易市场，所以也是必须要重视的。而且通常这类游戏都有比较多的海外份额，所以除了中英，在一些东南亚，阿拉伯等语种上也对模型提出了更多的要求。

### 对抗样本是什么
大家可能会好奇前面提到的对抗样本是什么，其实非常常见，比如
- 文本类有谐音梗、错别字、emoji 替代等
- 图文混排类有敏感内容隐藏在水印、拼图里
- 或者是AIGC 场景的提示词注入，模型越狱等，诱导模型输出违规内容等等。

如果说平台的安全防护是盾，那么用户就是矛，双方不断对抗，共同感知安全水位的变化。


## 业务方一般怎么选内容安全产品

1. 产品侧一般都会先参考当下应用安全标准以及行业平均水位，建立评估自身的评估维度：延迟、召回率/精准率、覆盖面、语种、合规性。

2. 产品方案调研：比如国内内容安全解决方案做得比较好的数美，易盾，腾讯，阿里，海外AWS，GCP等厂商，需要考虑的一般包括
    a. 模型部署的地区（境内/境外）影响延迟与合规，以及响应rt是多少，是否具备实时可用性
    b. 并发的支持度：毕竟很多时候是实时使用，尤其如果是llm流式输出做检测，一段话可能通过攒批方式检测可能就有十多次，高峰期并发量会非常突出
    c. 控制台功能完善度：包含样本检测、风控趋势报表，以及自定义词库与规则配置等。安全侧很多运营同学需要在控制台上测试数据，而且也需要通过控制台查看每天或者每周的风险类别数量和变化趋势，发现平台用户违禁类型类型变化，以及哪些维度还有待改进等等

3. 测试集构建：
    a. 一般都先会设定有多维度的风险类别，可能还会有二级或者三级类目
    b. 测试集一般都出自真实的用户数据，黑白样本比例可以按照当前平台的比例设定，同时黑样本最好覆盖的维度尽可能多样

5. 成本评估：一般需要计算各种使用量下的成本预算，比如按次调用的资源包价格，或者是按tokens调用的资源包价格等，同时可以考虑混合架构优化成本（小模型过滤 + 大模型精判）




## 结语

相信未来多模态检测将成为标配（文字 + 图片 + 视频 + 音频），AI和人工共同协作，降低错判率，提升处理效率。同时行业里矛与盾对抗将会长期存在，需要平台方持续迭代和自动化样本回流，不断优化改进，保障平台合规性，降低潜在风险。

内容安全不是锦上添花，而是平台的生命线。尤其在大模型和 AIGC 普及的今天，用户的创造力无限，对抗手法也层出不穷。如何在“安全”与“体验”之间找到新的平衡，是所有互联网平台接下来都要面对的核心问题。而且在大模型内容安全的背景下，谁应该为 AIGC 生成的违规内容负责？是开发者、平台还是用户？这不仅是技术问题，也是一个亟待社会和法律层面解决的伦理问题。
